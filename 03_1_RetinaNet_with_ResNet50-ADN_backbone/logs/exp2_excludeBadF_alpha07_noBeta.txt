[2024-04-29 04:00:42,287] torch.distributed.run: [WARNING] 
[2024-04-29 04:00:42,287] torch.distributed.run: [WARNING] *****************************************
[2024-04-29 04:00:42,287] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-29 04:00:42,287] torch.distributed.run: [WARNING] *****************************************
| distributed init (rank 0): env://
| distributed init (rank 1): env://
| distributed init (rank 2): env://
| distributed init (rank 3): env://
Namespace(data_path='/media/data/coco', dataset='coco', weights_path=None, clip_grad_norm=None, model='retinanet_resnet50_adn_fpn', device='cuda', batch_size=4, epochs=13, workers=8, opt='sgd', lr=0.01, momentum=0.9, weight_decay=0.0001, norm_weight_decay=None, lr_scheduler='multisteplr', lr_step_size=8, lr_steps=[8, 11], lr_gamma=0.1, print_freq=20, subpath_alpha=0.7, beta=0.9, output_dir='.', resume='', start_epoch=0, aspect_ratio_group_factor=3, rpn_score_thresh=None, trainable_backbone_layers=None, data_augmentation='hflip', sync_bn=False, test_only=False, use_deterministic_algorithms=False, world_size=4, dist_url='env://', weights=None, weights_backbone='/home/hslee/INU_RISE/02_AdaptiveDepthNetwork/pretrained/resnet50_adn_model_145.pth', amp=False, use_copypaste=False, backend='pil', use_v2=False, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Loading data
loading annotations into memory...
Done (t=8.25s)
creating index...
index created!
loading annotations into memory...
Done (t=0.26s)
creating index...
index created!
Creating data loaders
Using [0, 0.5, 0.6299605249474365, 0.7937005259840997, 1.0, 1.259921049894873, 1.5874010519681991, 2.0, inf] as bins for aspect ratio quantization
Count of instances per bin: [  104   982 24236  2332  8225 74466  5763  1158]
Creating model
Loading weights from /home/hslee/INU_RISE/02_AdaptiveDepthNetwork/pretrained/resnet50_adn_model_145.pth
trainable_backbone_layers : 3
extra_blocks: LastLevelP6P7(
  (p6): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
)
backbone.out_channels : 256
anchor_generator.num_anchors_per_location()[0] : 9
name : backbone.body.model.conv1.weight, requires_grad : False
name : backbone.body.model.layer1.0.conv1.weight, requires_grad : False
name : backbone.body.model.layer1.0.conv2.weight, requires_grad : False
name : backbone.body.model.layer1.0.conv3.weight, requires_grad : False
name : backbone.body.model.layer1.0.downsample.0.weight, requires_grad : False
name : backbone.body.model.layer1.1.conv1.weight, requires_grad : False
name : backbone.body.model.layer1.1.conv2.weight, requires_grad : False
name : backbone.body.model.layer1.1.conv3.weight, requires_grad : False
name : backbone.body.model.layer1.2.conv1.weight, requires_grad : False
name : backbone.body.model.layer1.2.conv2.weight, requires_grad : False
name : backbone.body.model.layer1.2.conv3.weight, requires_grad : False
name : backbone.body.model.layer2.0.conv1.weight, requires_grad : True
name : backbone.body.model.layer2.0.conv2.weight, requires_grad : True
name : backbone.body.model.layer2.0.conv3.weight, requires_grad : True
name : backbone.body.model.layer2.0.downsample.0.weight, requires_grad : True
name : backbone.body.model.layer2.1.conv1.weight, requires_grad : True
name : backbone.body.model.layer2.1.conv2.weight, requires_grad : True
name : backbone.body.model.layer2.1.conv3.weight, requires_grad : True
name : backbone.body.model.layer2.2.conv1.weight, requires_grad : True
name : backbone.body.model.layer2.2.conv2.weight, requires_grad : True
name : backbone.body.model.layer2.2.conv3.weight, requires_grad : True
name : backbone.body.model.layer2.3.conv1.weight, requires_grad : True
name : backbone.body.model.layer2.3.conv2.weight, requires_grad : True
name : backbone.body.model.layer2.3.conv3.weight, requires_grad : True
name : backbone.body.model.layer3.0.conv1.weight, requires_grad : True
name : backbone.body.model.layer3.0.conv2.weight, requires_grad : True
name : backbone.body.model.layer3.0.conv3.weight, requires_grad : True
name : backbone.body.model.layer3.0.downsample.0.weight, requires_grad : True
name : backbone.body.model.layer3.1.conv1.weight, requires_grad : True
name : backbone.body.model.layer3.1.conv2.weight, requires_grad : True
name : backbone.body.model.layer3.1.conv3.weight, requires_grad : True
name : backbone.body.model.layer3.2.conv1.weight, requires_grad : True
name : backbone.body.model.layer3.2.conv2.weight, requires_grad : True
name : backbone.body.model.layer3.2.conv3.weight, requires_grad : True
name : backbone.body.model.layer3.3.conv1.weight, requires_grad : True
name : backbone.body.model.layer3.3.conv2.weight, requires_grad : True
name : backbone.body.model.layer3.3.conv3.weight, requires_grad : True
name : backbone.body.model.layer3.4.conv1.weight, requires_grad : True
name : backbone.body.model.layer3.4.conv2.weight, requires_grad : True
name : backbone.body.model.layer3.4.conv3.weight, requires_grad : True
name : backbone.body.model.layer3.5.conv1.weight, requires_grad : True
name : backbone.body.model.layer3.5.conv2.weight, requires_grad : True
name : backbone.body.model.layer3.5.conv3.weight, requires_grad : True
name : backbone.body.model.layer4.0.conv1.weight, requires_grad : True
name : backbone.body.model.layer4.0.conv2.weight, requires_grad : True
name : backbone.body.model.layer4.0.conv3.weight, requires_grad : True
name : backbone.body.model.layer4.0.downsample.0.weight, requires_grad : True
name : backbone.body.model.layer4.1.conv1.weight, requires_grad : True
name : backbone.body.model.layer4.1.conv2.weight, requires_grad : True
name : backbone.body.model.layer4.1.conv3.weight, requires_grad : True
name : backbone.body.model.layer4.2.conv1.weight, requires_grad : True
name : backbone.body.model.layer4.2.conv2.weight, requires_grad : True
name : backbone.body.model.layer4.2.conv3.weight, requires_grad : True
name : backbone.fpn.inner_blocks.0.0.weight, requires_grad : True
name : backbone.fpn.inner_blocks.0.0.bias, requires_grad : True
name : backbone.fpn.inner_blocks.1.0.weight, requires_grad : True
name : backbone.fpn.inner_blocks.1.0.bias, requires_grad : True
name : backbone.fpn.inner_blocks.2.0.weight, requires_grad : True
name : backbone.fpn.inner_blocks.2.0.bias, requires_grad : True
name : backbone.fpn.layer_blocks.0.0.weight, requires_grad : True
name : backbone.fpn.layer_blocks.0.0.bias, requires_grad : True
name : backbone.fpn.layer_blocks.1.0.weight, requires_grad : True
name : backbone.fpn.layer_blocks.1.0.bias, requires_grad : True
name : backbone.fpn.layer_blocks.2.0.weight, requires_grad : True
name : backbone.fpn.layer_blocks.2.0.bias, requires_grad : True
name : backbone.fpn.extra_blocks.p6.weight, requires_grad : True
name : backbone.fpn.extra_blocks.p6.bias, requires_grad : True
name : backbone.fpn.extra_blocks.p7.weight, requires_grad : True
name : backbone.fpn.extra_blocks.p7.bias, requires_grad : True
name : head.classification_head.conv.0.0.weight, requires_grad : True
name : head.classification_head.conv.0.0.bias, requires_grad : True
name : head.classification_head.conv.1.0.weight, requires_grad : True
name : head.classification_head.conv.1.0.bias, requires_grad : True
name : head.classification_head.conv.2.0.weight, requires_grad : True
name : head.classification_head.conv.2.0.bias, requires_grad : True
name : head.classification_head.conv.3.0.weight, requires_grad : True
name : head.classification_head.conv.3.0.bias, requires_grad : True
name : head.classification_head.cls_logits.weight, requires_grad : True
name : head.classification_head.cls_logits.bias, requires_grad : True
name : head.regression_head.conv.0.0.weight, requires_grad : True
name : head.regression_head.conv.0.0.bias, requires_grad : True
name : head.regression_head.conv.1.0.weight, requires_grad : True
name : head.regression_head.conv.1.0.bias, requires_grad : True
name : head.regression_head.conv.2.0.weight, requires_grad : True
name : head.regression_head.conv.2.0.bias, requires_grad : True
name : head.regression_head.conv.3.0.weight, requires_grad : True
name : head.regression_head.conv.3.0.bias, requires_grad : True
name : head.regression_head.bbox_reg.weight, requires_grad : True
name : head.regression_head.bbox_reg.bias, requires_grad : True
model : DistributedDataParallel(
  (module): RetinaNet(
    (backbone): BackboneWithADNFPN(
      (body): IntermediateLayerGetter(
        (model): ResNet(
          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
          (bn1): FrozenBatchNorm2d(64, eps=1e-05)
          (relu): ReLU(inplace=True)
          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
          (layer1): SkippableSequentialBlocks(
            (0): Bottleneck(
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(64, eps=1e-05)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(64, eps=1e-05)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(256, eps=1e-05)
              (relu): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): FrozenBatchNorm2d(256, eps=1e-05)
              )
              (bn1_skip): FrozenBatchNorm2d(64, eps=1e-05)
              (bn2_skip): FrozenBatchNorm2d(64, eps=1e-05)
              (bn3_skip): FrozenBatchNorm2d(256, eps=1e-05)
            )
            (1): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(64, eps=1e-05)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(64, eps=1e-05)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(256, eps=1e-05)
              (relu): ReLU(inplace=True)
              (bn1_skip): FrozenBatchNorm2d(64, eps=1e-05)
              (bn2_skip): FrozenBatchNorm2d(64, eps=1e-05)
              (bn3_skip): FrozenBatchNorm2d(256, eps=1e-05)
            )
            (2): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(64, eps=1e-05)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(64, eps=1e-05)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(256, eps=1e-05)
              (relu): ReLU(inplace=True)
            )
          )
          (layer2): SkippableSequentialBlocks(
            (0): Bottleneck(
              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(128, eps=1e-05)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(128, eps=1e-05)
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(512, eps=1e-05)
              (relu): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): FrozenBatchNorm2d(512, eps=1e-05)
              )
              (bn1_skip): FrozenBatchNorm2d(128, eps=1e-05)
              (bn2_skip): FrozenBatchNorm2d(128, eps=1e-05)
              (bn3_skip): FrozenBatchNorm2d(512, eps=1e-05)
            )
            (1): Bottleneck(
              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(128, eps=1e-05)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(128, eps=1e-05)
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(512, eps=1e-05)
              (relu): ReLU(inplace=True)
              (bn1_skip): FrozenBatchNorm2d(128, eps=1e-05)
              (bn2_skip): FrozenBatchNorm2d(128, eps=1e-05)
              (bn3_skip): FrozenBatchNorm2d(512, eps=1e-05)
            )
            (2): Bottleneck(
              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(128, eps=1e-05)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(128, eps=1e-05)
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(512, eps=1e-05)
              (relu): ReLU(inplace=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(128, eps=1e-05)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(128, eps=1e-05)
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(512, eps=1e-05)
              (relu): ReLU(inplace=True)
            )
          )
          (layer3): SkippableSequentialBlocks(
            (0): Bottleneck(
              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(256, eps=1e-05)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(256, eps=1e-05)
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
              (relu): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): FrozenBatchNorm2d(1024, eps=1e-05)
              )
              (bn1_skip): FrozenBatchNorm2d(256, eps=1e-05)
              (bn2_skip): FrozenBatchNorm2d(256, eps=1e-05)
              (bn3_skip): FrozenBatchNorm2d(1024, eps=1e-05)
            )
            (1): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(256, eps=1e-05)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(256, eps=1e-05)
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
              (relu): ReLU(inplace=True)
              (bn1_skip): FrozenBatchNorm2d(256, eps=1e-05)
              (bn2_skip): FrozenBatchNorm2d(256, eps=1e-05)
              (bn3_skip): FrozenBatchNorm2d(1024, eps=1e-05)
            )
            (2): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(256, eps=1e-05)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(256, eps=1e-05)
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
              (relu): ReLU(inplace=True)
              (bn1_skip): FrozenBatchNorm2d(256, eps=1e-05)
              (bn2_skip): FrozenBatchNorm2d(256, eps=1e-05)
              (bn3_skip): FrozenBatchNorm2d(1024, eps=1e-05)
            )
            (3): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(256, eps=1e-05)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(256, eps=1e-05)
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
              (relu): ReLU(inplace=True)
            )
            (4): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(256, eps=1e-05)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(256, eps=1e-05)
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
              (relu): ReLU(inplace=True)
            )
            (5): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(256, eps=1e-05)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(256, eps=1e-05)
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
              (relu): ReLU(inplace=True)
            )
          )
          (layer4): SkippableSequentialBlocks(
            (0): Bottleneck(
              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(512, eps=1e-05)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(512, eps=1e-05)
              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(2048, eps=1e-05)
              (relu): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): FrozenBatchNorm2d(2048, eps=1e-05)
              )
              (bn1_skip): FrozenBatchNorm2d(512, eps=1e-05)
              (bn2_skip): FrozenBatchNorm2d(512, eps=1e-05)
              (bn3_skip): FrozenBatchNorm2d(2048, eps=1e-05)
            )
            (1): Bottleneck(
              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(512, eps=1e-05)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(512, eps=1e-05)
              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(2048, eps=1e-05)
              (relu): ReLU(inplace=True)
              (bn1_skip): FrozenBatchNorm2d(512, eps=1e-05)
              (bn2_skip): FrozenBatchNorm2d(512, eps=1e-05)
              (bn3_skip): FrozenBatchNorm2d(2048, eps=1e-05)
            )
            (2): Bottleneck(
              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(512, eps=1e-05)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(512, eps=1e-05)
              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(2048, eps=1e-05)
              (relu): ReLU(inplace=True)
            )
          )
        )
      )
      (fpn): FeaturePyramidNetwork(
        (inner_blocks): ModuleList(
          (0): Conv2dNormActivation(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (layer_blocks): ModuleList(
          (0-2): 3 x Conv2dNormActivation(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (extra_blocks): LastLevelP6P7(
          (p6): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        )
      )
    )
    (anchor_generator): AnchorGenerator()
    (head): RetinaNetHead(
      (classification_head): RetinaNetClassificationHead(
        (conv): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
          )
        )
        (cls_logits): Conv2d(256, 819, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (regression_head): RetinaNetRegressionHead(
        (conv): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
          )
        )
        (bbox_reg): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (transform): GeneralizedRCNNTransform(
        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        Resize(min_size=(800,), max_size=1333, mode='bilinear')
    )
  )
)
#Params :  34014999
33792599
num_skippable_stages : 4
skip_cfg_supernet : [False, False, False, False]
skip_cfg_basenet : [True, True, True, True]
Start training
Epoch: [0]  [   0/7329]  eta: 4:18:48  lr: 0.000020  real_losses_super: 2.1111 (2.1111)  real_losses_base: 2.1103 (2.1103)  losses_base: 2.1204 (2.1204)  kd_loss: 0.0100 (0.0100)  time: 2.1188  data: 0.8156  max mem: 9774
Epoch: [0]  [  20/7329]  eta: 1:01:32  lr: 0.000220  real_losses_super: 1.9519 (2.0056)  real_losses_base: 1.9518 (2.0055)  losses_base: 1.9717 (2.0204)  kd_loss: 0.0139 (0.0148)  time: 0.4245  data: 0.0062  max mem: 11152
Epoch: [0]  [  40/7329]  eta: 0:55:32  lr: 0.000420  real_losses_super: 1.9946 (2.0267)  real_losses_base: 1.9952 (2.0267)  losses_base: 2.0089 (2.0410)  kd_loss: 0.0131 (0.0143)  time: 0.4067  data: 0.0049  max mem: 11154
Epoch: [0]  [  60/7329]  eta: 0:52:59  lr: 0.000619  real_losses_super: 1.9595 (2.0066)  real_losses_base: 1.9597 (2.0066)  losses_base: 1.9700 (2.0198)  kd_loss: 0.0103 (0.0132)  time: 0.3967  data: 0.0048  max mem: 11154
Epoch: [0]  [  80/7329]  eta: 0:51:45  lr: 0.000819  real_losses_super: 1.9276 (2.0008)  real_losses_base: 1.9266 (2.0008)  losses_base: 1.9363 (2.0131)  kd_loss: 0.0099 (0.0123)  time: 0.4010  data: 0.0047  max mem: 11204
Epoch: [0]  [ 100/7329]  eta: 0:50:50  lr: 0.001019  real_losses_super: 1.9572 (2.0010)  real_losses_base: 1.9563 (2.0010)  losses_base: 1.9643 (2.0126)  kd_loss: 0.0080 (0.0116)  time: 0.3960  data: 0.0048  max mem: 11204
Epoch: [0]  [ 120/7329]  eta: 0:50:10  lr: 0.001219  real_losses_super: 1.9492 (1.9945)  real_losses_base: 1.9495 (1.9945)  losses_base: 1.9549 (2.0057)  kd_loss: 0.0091 (0.0112)  time: 0.3953  data: 0.0048  max mem: 11230
Epoch: [0]  [ 140/7329]  eta: 0:49:37  lr: 0.001419  real_losses_super: 1.9750 (1.9938)  real_losses_base: 1.9757 (1.9939)  losses_base: 1.9849 (2.0046)  kd_loss: 0.0075 (0.0108)  time: 0.3934  data: 0.0050  max mem: 11230
Epoch: [0]  [ 160/7329]  eta: 0:49:09  lr: 0.001618  real_losses_super: 1.9216 (1.9899)  real_losses_base: 1.9217 (1.9900)  losses_base: 1.9302 (2.0004)  kd_loss: 0.0084 (0.0104)  time: 0.3924  data: 0.0051  max mem: 11230
Epoch: [0]  [ 180/7329]  eta: 0:48:42  lr: 0.001818  real_losses_super: 1.9043 (1.9864)  real_losses_base: 1.9050 (1.9864)  losses_base: 1.9129 (1.9967)  kd_loss: 0.0085 (0.0102)  time: 0.3871  data: 0.0053  max mem: 11230
Epoch: [0]  [ 200/7329]  eta: 0:48:18  lr: 0.002018  real_losses_super: 1.9561 (1.9828)  real_losses_base: 1.9584 (1.9830)  losses_base: 1.9669 (1.9931)  kd_loss: 0.0091 (0.0102)  time: 0.3874  data: 0.0052  max mem: 11230
Epoch: [0]  [ 220/7329]  eta: 0:48:01  lr: 0.002218  real_losses_super: 1.9442 (1.9821)  real_losses_base: 1.9452 (1.9822)  losses_base: 1.9634 (1.9925)  kd_loss: 0.0113 (0.0103)  time: 0.3926  data: 0.0049  max mem: 11235
Epoch: [0]  [ 240/7329]  eta: 0:47:42  lr: 0.002418  real_losses_super: 1.8852 (1.9768)  real_losses_base: 1.8860 (1.9771)  losses_base: 1.9027 (1.9880)  kd_loss: 0.0158 (0.0110)  time: 0.3864  data: 0.0048  max mem: 11235
Epoch: [0]  [ 260/7329]  eta: 0:47:27  lr: 0.002617  real_losses_super: 1.8716 (1.9714)  real_losses_base: 1.8754 (1.9717)  losses_base: 1.9493 (1.9857)  kd_loss: 0.0414 (0.0140)  time: 0.3919  data: 0.0049  max mem: 11235
Epoch: [0]  [ 280/7329]  eta: 0:47:13  lr: 0.002817  real_losses_super: 1.8616 (1.9655)  real_losses_base: 1.8580 (1.9656)  losses_base: 1.9140 (1.9826)  kd_loss: 0.0516 (0.0170)  time: 0.3897  data: 0.0045  max mem: 11235
Epoch: [0]  [ 300/7329]  eta: 0:47:03  lr: 0.003017  real_losses_super: 1.9355 (1.9658)  real_losses_base: 1.9350 (1.9658)  losses_base: 1.9488 (1.9826)  kd_loss: 0.0138 (0.0168)  time: 0.3978  data: 0.0048  max mem: 11235
Epoch: [0]  [ 320/7329]  eta: 0:46:49  lr: 0.003217  real_losses_super: 1.9101 (1.9633)  real_losses_base: 1.9095 (1.9632)  losses_base: 1.9309 (1.9801)  kd_loss: 0.0160 (0.0168)  time: 0.3893  data: 0.0048  max mem: 11235
Epoch: [0]  [ 340/7329]  eta: 0:46:36  lr: 0.003417  real_losses_super: 1.9009 (1.9620)  real_losses_base: 1.9046 (1.9620)  losses_base: 1.9282 (1.9792)  kd_loss: 0.0214 (0.0172)  time: 0.3875  data: 0.0047  max mem: 11235
Epoch: [0]  [ 360/7329]  eta: 0:46:24  lr: 0.003616  real_losses_super: 1.9188 (1.9594)  real_losses_base: 1.9184 (1.9594)  losses_base: 1.9363 (1.9765)  kd_loss: 0.0125 (0.0171)  time: 0.3910  data: 0.0047  max mem: 11235
Epoch: [0]  [ 380/7329]  eta: 0:46:13  lr: 0.003816  real_losses_super: 1.9032 (1.9596)  real_losses_base: 1.9048 (1.9596)  losses_base: 1.9233 (1.9766)  kd_loss: 0.0137 (0.0170)  time: 0.3906  data: 0.0053  max mem: 11235
Epoch: [0]  [ 400/7329]  eta: 0:46:01  lr: 0.004016  real_losses_super: 1.9194 (1.9599)  real_losses_base: 1.9209 (1.9600)  losses_base: 1.9421 (1.9775)  kd_loss: 0.0221 (0.0175)  time: 0.3861  data: 0.0044  max mem: 11235
Epoch: [0]  [ 420/7329]  eta: 0:45:50  lr: 0.004216  real_losses_super: 1.9376 (1.9585)  real_losses_base: 1.9364 (1.9586)  losses_base: 1.9511 (1.9760)  kd_loss: 0.0146 (0.0174)  time: 0.3918  data: 0.0044  max mem: 11235
Epoch: [0]  [ 440/7329]  eta: 0:45:37  lr: 0.004416  real_losses_super: 1.9045 (1.9572)  real_losses_base: 1.9046 (1.9573)  losses_base: 1.9173 (1.9745)  kd_loss: 0.0141 (0.0172)  time: 0.3809  data: 0.0048  max mem: 11235
Epoch: [0]  [ 460/7329]  eta: 0:45:28  lr: 0.004615  real_losses_super: 1.8995 (1.9561)  real_losses_base: 1.9010 (1.9563)  losses_base: 1.9095 (1.9732)  kd_loss: 0.0088 (0.0169)  time: 0.3944  data: 0.0048  max mem: 11251
Epoch: [0]  [ 480/7329]  eta: 0:45:19  lr: 0.004815  real_losses_super: 1.8690 (1.9539)  real_losses_base: 1.8711 (1.9541)  losses_base: 1.8852 (1.9707)  kd_loss: 0.0061 (0.0166)  time: 0.3930  data: 0.0047  max mem: 11251
Epoch: [0]  [ 500/7329]  eta: 0:45:09  lr: 0.005015  real_losses_super: 1.8700 (1.9508)  real_losses_base: 1.8721 (1.9509)  losses_base: 1.8757 (1.9670)  kd_loss: 0.0027 (0.0161)  time: 0.3881  data: 0.0053  max mem: 11251
Epoch: [0]  [ 520/7329]  eta: 0:44:57  lr: 0.005215  real_losses_super: 1.7767 (1.9479)  real_losses_base: 1.7817 (1.9481)  losses_base: 1.7823 (1.9636)  kd_loss: 0.0010 (0.0155)  time: 0.3840  data: 0.0044  max mem: 11251
Epoch: [0]  [ 540/7329]  eta: 0:44:50  lr: 0.005415  real_losses_super: 1.6668 (1.9383)  real_losses_base: 1.6674 (1.9386)  losses_base: 1.6685 (1.9536)  kd_loss: 0.0009 (0.0150)  time: 0.3981  data: 0.0043  max mem: 11251
Epoch: [0]  [ 560/7329]  eta: 0:44:40  lr: 0.005614  real_losses_super: 1.6745 (1.9282)  real_losses_base: 1.6705 (1.9288)  losses_base: 1.6723 (1.9433)  kd_loss: 0.0004 (0.0145)  time: 0.3874  data: 0.0048  max mem: 11251
Epoch: [0]  [ 580/7329]  eta: 0:44:30  lr: 0.005814  real_losses_super: 1.5595 (1.9167)  real_losses_base: 1.5664 (1.9175)  losses_base: 1.5723 (1.9316)  kd_loss: 0.0028 (0.0142)  time: 0.3862  data: 0.0047  max mem: 11251
Epoch: [0]  [ 600/7329]  eta: 0:44:21  lr: 0.006014  real_losses_super: 1.5240 (1.9042)  real_losses_base: 1.5332 (1.9051)  losses_base: 1.5346 (1.9191)  kd_loss: 0.0048 (0.0140)  time: 0.3926  data: 0.0045  max mem: 11251
Epoch: [0]  [ 620/7329]  eta: 0:44:14  lr: 0.006214  real_losses_super: 1.5198 (1.8914)  real_losses_base: 1.5339 (1.8925)  losses_base: 1.5420 (1.9063)  kd_loss: 0.0055 (0.0138)  time: 0.3987  data: 0.0045  max mem: 11251
Epoch: [0]  [ 640/7329]  eta: 0:44:04  lr: 0.006414  real_losses_super: 1.6101 (1.8836)  real_losses_base: 1.6183 (1.8846)  losses_base: 1.6482 (1.8987)  kd_loss: 0.0152 (0.0141)  time: 0.3882  data: 0.0049  max mem: 11251
Epoch: [0]  [ 660/7329]  eta: 0:43:55  lr: 0.006613  real_losses_super: 1.6430 (1.8754)  real_losses_base: 1.6479 (1.8765)  losses_base: 1.6658 (1.8909)  kd_loss: 0.0233 (0.0144)  time: 0.3888  data: 0.0051  max mem: 11256
Epoch: [0]  [ 680/7329]  eta: 0:43:46  lr: 0.006813  real_losses_super: 1.6152 (1.8679)  real_losses_base: 1.6152 (1.8690)  losses_base: 1.6265 (1.8837)  kd_loss: 0.0235 (0.0147)  time: 0.3869  data: 0.0049  max mem: 11256
Epoch: [0]  [ 700/7329]  eta: 0:43:36  lr: 0.007013  real_losses_super: 1.5886 (1.8595)  real_losses_base: 1.6008 (1.8607)  losses_base: 1.6131 (1.8753)  kd_loss: 0.0091 (0.0146)  time: 0.3859  data: 0.0047  max mem: 11256
Epoch: [0]  [ 720/7329]  eta: 0:43:27  lr: 0.007213  real_losses_super: 1.7901 (1.8579)  real_losses_base: 1.7804 (1.8588)  losses_base: 1.8293 (1.8749)  kd_loss: 0.0422 (0.0160)  time: 0.3866  data: 0.0049  max mem: 11256
Epoch: [0]  [ 740/7329]  eta: 0:43:19  lr: 0.007413  real_losses_super: 1.7241 (1.8553)  real_losses_base: 1.7305 (1.8564)  losses_base: 1.7985 (1.8731)  kd_loss: 0.0410 (0.0167)  time: 0.3942  data: 0.0048  max mem: 11256
Epoch: [0]  [ 760/7329]  eta: 0:43:10  lr: 0.007612  real_losses_super: 1.7861 (1.8556)  real_losses_base: 1.7948 (1.8562)  losses_base: 1.8669 (1.8746)  kd_loss: 0.0445 (0.0184)  time: 0.3927  data: 0.0050  max mem: 11256
Epoch: [0]  [ 780/7329]  eta: 0:43:02  lr: 0.007812  real_losses_super: 1.8512 (1.8574)  real_losses_base: 1.8457 (1.8581)  losses_base: 2.1891 (1.8908)  kd_loss: 0.2038 (0.0327)  time: 0.3919  data: 0.0050  max mem: 11256
real_loss_super is nan, stopping training
[2024-04-29 04:06:12,311] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 4052933) of binary: /home/hslee/anaconda3/bin/python
Traceback (most recent call last):
  File "/home/hslee/anaconda3/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.2.1', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_custom.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-04-29_04:06:12
  host      : ampere
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 4052934)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-04-29_04:06:12
  host      : ampere
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 4052935)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-04-29_04:06:12
  host      : ampere
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 4052936)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-04-29_04:06:12
  host      : ampere
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 4052933)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
