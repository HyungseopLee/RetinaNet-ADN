[2024-04-28 21:23:18,448] torch.distributed.run: [WARNING] 
[2024-04-28 21:23:18,448] torch.distributed.run: [WARNING] *****************************************
[2024-04-28 21:23:18,448] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-28 21:23:18,448] torch.distributed.run: [WARNING] *****************************************
| distributed init (rank 0): env://
| distributed init (rank 3): env://
| distributed init (rank 1): env://
| distributed init (rank 2): env://
Namespace(data_path='/media/data/coco', dataset='coco', weights_path=None, clip_grad_norm=None, model='retinanet_resnet50_adn_fpn', device='cuda', batch_size=4, epochs=13, workers=8, opt='sgd', lr=0.01, momentum=0.9, weight_decay=0.0001, norm_weight_decay=None, lr_scheduler='multisteplr', lr_step_size=8, lr_steps=[8, 11], lr_gamma=0.1, print_freq=20, subpath_alpha=0.5, beta=0.9, output_dir='.', resume='', start_epoch=0, aspect_ratio_group_factor=3, rpn_score_thresh=None, trainable_backbone_layers=None, data_augmentation='hflip', sync_bn=False, test_only=False, use_deterministic_algorithms=False, world_size=4, dist_url='env://', weights=None, weights_backbone='/home/hslee/INU_RISE/02_AdaptiveDepthNetwork/pretrained/resnet50_adn_model_145.pth', amp=False, use_copypaste=False, backend='pil', use_v2=False, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Loading data
loading annotations into memory...
Done (t=8.45s)
creating index...
index created!
loading annotations into memory...
Done (t=0.26s)
creating index...
index created!
Creating data loaders
Using [0, 0.5, 0.6299605249474365, 0.7937005259840997, 1.0, 1.259921049894873, 1.5874010519681991, 2.0, inf] as bins for aspect ratio quantization
Count of instances per bin: [  104   982 24236  2332  8225 74466  5763  1158]
Creating model
Loading weights from /home/hslee/INU_RISE/02_AdaptiveDepthNetwork/pretrained/resnet50_adn_model_145.pth
trainable_backbone_layers : 3
extra_blocks: LastLevelP6P7(
  (p6): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
)
backbone.out_channels : 256
anchor_generator.num_anchors_per_location()[0] : 9
name : backbone.body.model.conv1.weight, requires_grad : False
name : backbone.body.model.layer1.0.conv1.weight, requires_grad : False
name : backbone.body.model.layer1.0.conv2.weight, requires_grad : False
name : backbone.body.model.layer1.0.conv3.weight, requires_grad : False
name : backbone.body.model.layer1.0.downsample.0.weight, requires_grad : False
name : backbone.body.model.layer1.1.conv1.weight, requires_grad : False
name : backbone.body.model.layer1.1.conv2.weight, requires_grad : False
name : backbone.body.model.layer1.1.conv3.weight, requires_grad : False
name : backbone.body.model.layer1.2.conv1.weight, requires_grad : False
name : backbone.body.model.layer1.2.conv2.weight, requires_grad : False
name : backbone.body.model.layer1.2.conv3.weight, requires_grad : False
name : backbone.body.model.layer2.0.conv1.weight, requires_grad : True
name : backbone.body.model.layer2.0.conv2.weight, requires_grad : True
name : backbone.body.model.layer2.0.conv3.weight, requires_grad : True
name : backbone.body.model.layer2.0.downsample.0.weight, requires_grad : True
name : backbone.body.model.layer2.1.conv1.weight, requires_grad : True
name : backbone.body.model.layer2.1.conv2.weight, requires_grad : True
name : backbone.body.model.layer2.1.conv3.weight, requires_grad : True
name : backbone.body.model.layer2.2.conv1.weight, requires_grad : True
name : backbone.body.model.layer2.2.conv2.weight, requires_grad : True
name : backbone.body.model.layer2.2.conv3.weight, requires_grad : True
name : backbone.body.model.layer2.3.conv1.weight, requires_grad : True
name : backbone.body.model.layer2.3.conv2.weight, requires_grad : True
name : backbone.body.model.layer2.3.conv3.weight, requires_grad : True
name : backbone.body.model.layer3.0.conv1.weight, requires_grad : True
name : backbone.body.model.layer3.0.conv2.weight, requires_grad : True
name : backbone.body.model.layer3.0.conv3.weight, requires_grad : True
name : backbone.body.model.layer3.0.downsample.0.weight, requires_grad : True
name : backbone.body.model.layer3.1.conv1.weight, requires_grad : True
name : backbone.body.model.layer3.1.conv2.weight, requires_grad : True
name : backbone.body.model.layer3.1.conv3.weight, requires_grad : True
name : backbone.body.model.layer3.2.conv1.weight, requires_grad : True
name : backbone.body.model.layer3.2.conv2.weight, requires_grad : True
name : backbone.body.model.layer3.2.conv3.weight, requires_grad : True
name : backbone.body.model.layer3.3.conv1.weight, requires_grad : True
name : backbone.body.model.layer3.3.conv2.weight, requires_grad : True
name : backbone.body.model.layer3.3.conv3.weight, requires_grad : True
name : backbone.body.model.layer3.4.conv1.weight, requires_grad : True
name : backbone.body.model.layer3.4.conv2.weight, requires_grad : True
name : backbone.body.model.layer3.4.conv3.weight, requires_grad : True
name : backbone.body.model.layer3.5.conv1.weight, requires_grad : True
name : backbone.body.model.layer3.5.conv2.weight, requires_grad : True
name : backbone.body.model.layer3.5.conv3.weight, requires_grad : True
name : backbone.body.model.layer4.0.conv1.weight, requires_grad : True
name : backbone.body.model.layer4.0.conv2.weight, requires_grad : True
name : backbone.body.model.layer4.0.conv3.weight, requires_grad : True
name : backbone.body.model.layer4.0.downsample.0.weight, requires_grad : True
name : backbone.body.model.layer4.1.conv1.weight, requires_grad : True
name : backbone.body.model.layer4.1.conv2.weight, requires_grad : True
name : backbone.body.model.layer4.1.conv3.weight, requires_grad : True
name : backbone.body.model.layer4.2.conv1.weight, requires_grad : True
name : backbone.body.model.layer4.2.conv2.weight, requires_grad : True
name : backbone.body.model.layer4.2.conv3.weight, requires_grad : True
name : backbone.fpn.inner_blocks.0.0.weight, requires_grad : True
name : backbone.fpn.inner_blocks.0.0.bias, requires_grad : True
name : backbone.fpn.inner_blocks.1.0.weight, requires_grad : True
name : backbone.fpn.inner_blocks.1.0.bias, requires_grad : True
name : backbone.fpn.inner_blocks.2.0.weight, requires_grad : True
name : backbone.fpn.inner_blocks.2.0.bias, requires_grad : True
name : backbone.fpn.layer_blocks.0.0.weight, requires_grad : True
name : backbone.fpn.layer_blocks.0.0.bias, requires_grad : True
name : backbone.fpn.layer_blocks.1.0.weight, requires_grad : True
name : backbone.fpn.layer_blocks.1.0.bias, requires_grad : True
name : backbone.fpn.layer_blocks.2.0.weight, requires_grad : True
name : backbone.fpn.layer_blocks.2.0.bias, requires_grad : True
name : backbone.fpn.extra_blocks.p6.weight, requires_grad : True
name : backbone.fpn.extra_blocks.p6.bias, requires_grad : True
name : backbone.fpn.extra_blocks.p7.weight, requires_grad : True
name : backbone.fpn.extra_blocks.p7.bias, requires_grad : True
name : head.classification_head.conv.0.0.weight, requires_grad : True
name : head.classification_head.conv.0.0.bias, requires_grad : True
name : head.classification_head.conv.1.0.weight, requires_grad : True
name : head.classification_head.conv.1.0.bias, requires_grad : True
name : head.classification_head.conv.2.0.weight, requires_grad : True
name : head.classification_head.conv.2.0.bias, requires_grad : True
name : head.classification_head.conv.3.0.weight, requires_grad : True
name : head.classification_head.conv.3.0.bias, requires_grad : True
name : head.classification_head.cls_logits.weight, requires_grad : True
name : head.classification_head.cls_logits.bias, requires_grad : True
name : head.regression_head.conv.0.0.weight, requires_grad : True
name : head.regression_head.conv.0.0.bias, requires_grad : True
name : head.regression_head.conv.1.0.weight, requires_grad : True
name : head.regression_head.conv.1.0.bias, requires_grad : True
name : head.regression_head.conv.2.0.weight, requires_grad : True
name : head.regression_head.conv.2.0.bias, requires_grad : True
name : head.regression_head.conv.3.0.weight, requires_grad : True
name : head.regression_head.conv.3.0.bias, requires_grad : True
name : head.regression_head.bbox_reg.weight, requires_grad : True
name : head.regression_head.bbox_reg.bias, requires_grad : True
model : DistributedDataParallel(
  (module): RetinaNet(
    (backbone): BackboneWithADNFPN(
      (body): IntermediateLayerGetter(
        (model): ResNet(
          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
          (bn1): FrozenBatchNorm2d(64, eps=1e-05)
          (relu): ReLU(inplace=True)
          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
          (layer1): SkippableSequentialBlocks(
            (0): Bottleneck(
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(64, eps=1e-05)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(64, eps=1e-05)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(256, eps=1e-05)
              (relu): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): FrozenBatchNorm2d(256, eps=1e-05)
              )
              (bn1_skip): FrozenBatchNorm2d(64, eps=1e-05)
              (bn2_skip): FrozenBatchNorm2d(64, eps=1e-05)
              (bn3_skip): FrozenBatchNorm2d(256, eps=1e-05)
            )
            (1): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(64, eps=1e-05)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(64, eps=1e-05)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(256, eps=1e-05)
              (relu): ReLU(inplace=True)
              (bn1_skip): FrozenBatchNorm2d(64, eps=1e-05)
              (bn2_skip): FrozenBatchNorm2d(64, eps=1e-05)
              (bn3_skip): FrozenBatchNorm2d(256, eps=1e-05)
            )
            (2): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(64, eps=1e-05)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(64, eps=1e-05)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(256, eps=1e-05)
              (relu): ReLU(inplace=True)
            )
          )
          (layer2): SkippableSequentialBlocks(
            (0): Bottleneck(
              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(128, eps=1e-05)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(128, eps=1e-05)
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(512, eps=1e-05)
              (relu): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): FrozenBatchNorm2d(512, eps=1e-05)
              )
              (bn1_skip): FrozenBatchNorm2d(128, eps=1e-05)
              (bn2_skip): FrozenBatchNorm2d(128, eps=1e-05)
              (bn3_skip): FrozenBatchNorm2d(512, eps=1e-05)
            )
            (1): Bottleneck(
              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(128, eps=1e-05)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(128, eps=1e-05)
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(512, eps=1e-05)
              (relu): ReLU(inplace=True)
              (bn1_skip): FrozenBatchNorm2d(128, eps=1e-05)
              (bn2_skip): FrozenBatchNorm2d(128, eps=1e-05)
              (bn3_skip): FrozenBatchNorm2d(512, eps=1e-05)
            )
            (2): Bottleneck(
              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(128, eps=1e-05)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(128, eps=1e-05)
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(512, eps=1e-05)
              (relu): ReLU(inplace=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(128, eps=1e-05)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(128, eps=1e-05)
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(512, eps=1e-05)
              (relu): ReLU(inplace=True)
            )
          )
          (layer3): SkippableSequentialBlocks(
            (0): Bottleneck(
              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(256, eps=1e-05)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(256, eps=1e-05)
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
              (relu): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): FrozenBatchNorm2d(1024, eps=1e-05)
              )
              (bn1_skip): FrozenBatchNorm2d(256, eps=1e-05)
              (bn2_skip): FrozenBatchNorm2d(256, eps=1e-05)
              (bn3_skip): FrozenBatchNorm2d(1024, eps=1e-05)
            )
            (1): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(256, eps=1e-05)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(256, eps=1e-05)
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
              (relu): ReLU(inplace=True)
              (bn1_skip): FrozenBatchNorm2d(256, eps=1e-05)
              (bn2_skip): FrozenBatchNorm2d(256, eps=1e-05)
              (bn3_skip): FrozenBatchNorm2d(1024, eps=1e-05)
            )
            (2): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(256, eps=1e-05)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(256, eps=1e-05)
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
              (relu): ReLU(inplace=True)
              (bn1_skip): FrozenBatchNorm2d(256, eps=1e-05)
              (bn2_skip): FrozenBatchNorm2d(256, eps=1e-05)
              (bn3_skip): FrozenBatchNorm2d(1024, eps=1e-05)
            )
            (3): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(256, eps=1e-05)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(256, eps=1e-05)
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
              (relu): ReLU(inplace=True)
            )
            (4): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(256, eps=1e-05)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(256, eps=1e-05)
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
              (relu): ReLU(inplace=True)
            )
            (5): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(256, eps=1e-05)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(256, eps=1e-05)
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
              (relu): ReLU(inplace=True)
            )
          )
          (layer4): SkippableSequentialBlocks(
            (0): Bottleneck(
              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(512, eps=1e-05)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(512, eps=1e-05)
              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(2048, eps=1e-05)
              (relu): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): FrozenBatchNorm2d(2048, eps=1e-05)
              )
              (bn1_skip): FrozenBatchNorm2d(512, eps=1e-05)
              (bn2_skip): FrozenBatchNorm2d(512, eps=1e-05)
              (bn3_skip): FrozenBatchNorm2d(2048, eps=1e-05)
            )
            (1): Bottleneck(
              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(512, eps=1e-05)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(512, eps=1e-05)
              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(2048, eps=1e-05)
              (relu): ReLU(inplace=True)
              (bn1_skip): FrozenBatchNorm2d(512, eps=1e-05)
              (bn2_skip): FrozenBatchNorm2d(512, eps=1e-05)
              (bn3_skip): FrozenBatchNorm2d(2048, eps=1e-05)
            )
            (2): Bottleneck(
              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(512, eps=1e-05)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(512, eps=1e-05)
              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(2048, eps=1e-05)
              (relu): ReLU(inplace=True)
            )
          )
        )
      )
      (fpn): FeaturePyramidNetwork(
        (inner_blocks): ModuleList(
          (0): Conv2dNormActivation(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (layer_blocks): ModuleList(
          (0-2): 3 x Conv2dNormActivation(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (extra_blocks): LastLevelP6P7(
          (p6): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        )
      )
    )
    (anchor_generator): AnchorGenerator()
    (head): RetinaNetHead(
      (classification_head): RetinaNetClassificationHead(
        (conv): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
          )
        )
        (cls_logits): Conv2d(256, 819, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (regression_head): RetinaNetRegressionHead(
        (conv): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
          )
        )
        (bbox_reg): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (transform): GeneralizedRCNNTransform(
        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        Resize(min_size=(800,), max_size=1333, mode='bilinear')
    )
  )
)
#Params :  34014999
33792599
num_skippable_stages : 4
skip_cfg_supernet : [False, False, False, False]
skip_cfg_basenet : [True, True, True, True]
Start training
Epoch: [0]  [   0/7329]  eta: 4:13:19  lr: 0.000020  real_losses_super: 2.1155 (2.1155)  real_losses_base: 2.1146 (2.1146)  losses_base: 2.1240 (2.1240)  kd_loss: 0.0094 (0.0094)  time: 2.0739  data: 0.8085  max mem: 9774
Epoch: [0]  [  20/7329]  eta: 1:01:07  lr: 0.000220  real_losses_super: 1.9519 (2.0053)  real_losses_base: 1.9523 (2.0052)  losses_base: 1.9656 (2.0195)  kd_loss: 0.0134 (0.0143)  time: 0.4232  data: 0.0065  max mem: 11152
Epoch: [0]  [  40/7329]  eta: 0:55:03  lr: 0.000420  real_losses_super: 1.9974 (2.0277)  real_losses_base: 1.9969 (2.0276)  losses_base: 2.0096 (2.0411)  kd_loss: 0.0123 (0.0135)  time: 0.4021  data: 0.0047  max mem: 11155
Epoch: [0]  [  60/7329]  eta: 0:52:40  lr: 0.000619  real_losses_super: 1.9590 (2.0075)  real_losses_base: 1.9584 (2.0073)  losses_base: 1.9680 (2.0196)  kd_loss: 0.0097 (0.0123)  time: 0.3970  data: 0.0048  max mem: 11155
Epoch: [0]  [  80/7329]  eta: 0:51:31  lr: 0.000819  real_losses_super: 1.9240 (2.0010)  real_losses_base: 1.9242 (2.0008)  losses_base: 1.9343 (2.0122)  kd_loss: 0.0085 (0.0113)  time: 0.4010  data: 0.0048  max mem: 11208
Epoch: [0]  [ 100/7329]  eta: 0:50:35  lr: 0.001019  real_losses_super: 1.9528 (2.0020)  real_losses_base: 1.9528 (2.0019)  losses_base: 1.9597 (2.0126)  kd_loss: 0.0073 (0.0107)  time: 0.3936  data: 0.0049  max mem: 11208
Epoch: [0]  [ 120/7329]  eta: 0:49:58  lr: 0.001219  real_losses_super: 1.9527 (1.9955)  real_losses_base: 1.9528 (1.9954)  losses_base: 1.9584 (2.0057)  kd_loss: 0.0088 (0.0103)  time: 0.3957  data: 0.0051  max mem: 11226
Epoch: [0]  [ 140/7329]  eta: 0:49:25  lr: 0.001419  real_losses_super: 1.9736 (1.9950)  real_losses_base: 1.9736 (1.9950)  losses_base: 1.9792 (2.0048)  kd_loss: 0.0070 (0.0099)  time: 0.3920  data: 0.0046  max mem: 11226
Epoch: [0]  [ 160/7329]  eta: 0:48:58  lr: 0.001618  real_losses_super: 1.9170 (1.9911)  real_losses_base: 1.9163 (1.9910)  losses_base: 1.9246 (2.0006)  kd_loss: 0.0074 (0.0096)  time: 0.3916  data: 0.0050  max mem: 11226
Epoch: [0]  [ 180/7329]  eta: 0:48:31  lr: 0.001818  real_losses_super: 1.9052 (1.9877)  real_losses_base: 1.9060 (1.9877)  losses_base: 1.9136 (1.9972)  kd_loss: 0.0088 (0.0095)  time: 0.3856  data: 0.0047  max mem: 11226
Epoch: [0]  [ 200/7329]  eta: 0:48:08  lr: 0.002018  real_losses_super: 1.9599 (1.9839)  real_losses_base: 1.9603 (1.9839)  losses_base: 1.9702 (1.9937)  kd_loss: 0.0118 (0.0098)  time: 0.3861  data: 0.0046  max mem: 11226
Epoch: [0]  [ 220/7329]  eta: 0:47:51  lr: 0.002218  real_losses_super: 1.9428 (1.9829)  real_losses_base: 1.9401 (1.9829)  losses_base: 1.9667 (1.9932)  kd_loss: 0.0146 (0.0103)  time: 0.3924  data: 0.0047  max mem: 11232
Epoch: [0]  [ 240/7329]  eta: 0:47:33  lr: 0.002418  real_losses_super: 1.8842 (1.9775)  real_losses_base: 1.8848 (1.9775)  losses_base: 1.9003 (1.9890)  kd_loss: 0.0210 (0.0115)  time: 0.3855  data: 0.0047  max mem: 11232
Epoch: [0]  [ 260/7329]  eta: 0:47:19  lr: 0.002617  real_losses_super: 1.8720 (1.9709)  real_losses_base: 1.8704 (1.9709)  losses_base: 1.9183 (1.9850)  kd_loss: 0.0343 (0.0141)  time: 0.3921  data: 0.0051  max mem: 11233
Epoch: [0]  [ 280/7329]  eta: 0:47:05  lr: 0.002817  real_losses_super: 1.8807 (1.9659)  real_losses_base: 1.8848 (1.9661)  losses_base: 1.8962 (1.9802)  kd_loss: 0.0136 (0.0141)  time: 0.3891  data: 0.0049  max mem: 11233
Epoch: [0]  [ 300/7329]  eta: 0:46:55  lr: 0.003017  real_losses_super: 1.8745 (1.9597)  real_losses_base: 1.8771 (1.9604)  losses_base: 1.8932 (1.9753)  kd_loss: 0.0197 (0.0149)  time: 0.3971  data: 0.0049  max mem: 11233
Epoch: [0]  [ 320/7329]  eta: 0:46:41  lr: 0.003217  real_losses_super: 1.7956 (1.9492)  real_losses_base: 1.8023 (1.9502)  losses_base: 1.8157 (1.9650)  kd_loss: 0.0143 (0.0149)  time: 0.3874  data: 0.0047  max mem: 11233
Epoch: [0]  [ 340/7329]  eta: 0:46:28  lr: 0.003417  real_losses_super: 1.7641 (1.9400)  real_losses_base: 1.7693 (1.9410)  losses_base: 1.7787 (1.9558)  kd_loss: 0.0129 (0.0148)  time: 0.3874  data: 0.0044  max mem: 11233
Epoch: [0]  [ 360/7329]  eta: 0:46:17  lr: 0.003616  real_losses_super: 1.7094 (1.9270)  real_losses_base: 1.7139 (1.9282)  losses_base: 1.7281 (1.9429)  kd_loss: 0.0117 (0.0147)  time: 0.3905  data: 0.0043  max mem: 11233
Epoch: [0]  [ 380/7329]  eta: 0:46:06  lr: 0.003816  real_losses_super: 1.7123 (1.9170)  real_losses_base: 1.7125 (1.9181)  losses_base: 1.7260 (1.9326)  kd_loss: 0.0101 (0.0145)  time: 0.3896  data: 0.0045  max mem: 11233
Epoch: [0]  [ 400/7329]  eta: 0:45:53  lr: 0.004016  real_losses_super: 1.6904 (1.9084)  real_losses_base: 1.6909 (1.9094)  losses_base: 1.7026 (1.9237)  kd_loss: 0.0102 (0.0143)  time: 0.3861  data: 0.0048  max mem: 11233
Epoch: [0]  [ 420/7329]  eta: 0:45:44  lr: 0.004216  real_losses_super: 1.6131 (1.8957)  real_losses_base: 1.6002 (1.8967)  losses_base: 1.6099 (1.9108)  kd_loss: 0.0097 (0.0141)  time: 0.3923  data: 0.0053  max mem: 11233
Epoch: [0]  [ 440/7329]  eta: 0:45:31  lr: 0.004416  real_losses_super: 1.6088 (1.8837)  real_losses_base: 1.6012 (1.8850)  losses_base: 1.6119 (1.8989)  kd_loss: 0.0097 (0.0139)  time: 0.3810  data: 0.0048  max mem: 11233
Epoch: [0]  [ 460/7329]  eta: 0:45:22  lr: 0.004615  real_losses_super: 1.6495 (1.8735)  real_losses_base: 1.6301 (1.8749)  losses_base: 1.6430 (1.8887)  kd_loss: 0.0099 (0.0138)  time: 0.3947  data: 0.0048  max mem: 11250
Epoch: [0]  [ 480/7329]  eta: 0:45:13  lr: 0.004815  real_losses_super: 1.7012 (1.8673)  real_losses_base: 1.7070 (1.8687)  losses_base: 1.7203 (1.8833)  kd_loss: 0.0198 (0.0146)  time: 0.3928  data: 0.0050  max mem: 11250
Epoch: [0]  [ 500/7329]  eta: 0:45:04  lr: 0.005015  real_losses_super: 1.6067 (1.8580)  real_losses_base: 1.6183 (1.8597)  losses_base: 1.6346 (1.8742)  kd_loss: 0.0138 (0.0146)  time: 0.3894  data: 0.0052  max mem: 11250
Epoch: [0]  [ 520/7329]  eta: 0:44:52  lr: 0.005215  real_losses_super: 1.6787 (1.8558)  real_losses_base: 1.6905 (1.8572)  losses_base: 1.7088 (1.8719)  kd_loss: 0.0122 (0.0147)  time: 0.3837  data: 0.0051  max mem: 11250
Epoch: [0]  [ 540/7329]  eta: 0:44:45  lr: 0.005415  real_losses_super: 1.6239 (1.8465)  real_losses_base: 1.5929 (1.8477)  losses_base: 1.6046 (1.8623)  kd_loss: 0.0079 (0.0145)  time: 0.3970  data: 0.0046  max mem: 11250
Epoch: [0]  [ 560/7329]  eta: 0:44:35  lr: 0.005614  real_losses_super: 1.5841 (1.8362)  real_losses_base: 1.5976 (1.8375)  losses_base: 1.6020 (1.8517)  kd_loss: 0.0048 (0.0142)  time: 0.3869  data: 0.0045  max mem: 11250
Epoch: [0]  [ 580/7329]  eta: 0:44:25  lr: 0.005814  real_losses_super: 1.5144 (1.8264)  real_losses_base: 1.5231 (1.8281)  losses_base: 1.5395 (1.8420)  kd_loss: 0.0052 (0.0139)  time: 0.3848  data: 0.0045  max mem: 11250
Epoch: [0]  [ 600/7329]  eta: 0:44:16  lr: 0.006014  real_losses_super: 1.4325 (1.8134)  real_losses_base: 1.4470 (1.8155)  losses_base: 1.4554 (1.8292)  kd_loss: 0.0055 (0.0137)  time: 0.3924  data: 0.0044  max mem: 11250
Epoch: [0]  [ 620/7329]  eta: 0:44:09  lr: 0.006214  real_losses_super: 1.4064 (1.8010)  real_losses_base: 1.4173 (1.8035)  losses_base: 1.4210 (1.8170)  kd_loss: 0.0054 (0.0135)  time: 0.3974  data: 0.0050  max mem: 11250
Epoch: [0]  [ 640/7329]  eta: 0:43:59  lr: 0.006414  real_losses_super: 1.4519 (1.7897)  real_losses_base: 1.4547 (1.7924)  losses_base: 1.4600 (1.8058)  kd_loss: 0.0071 (0.0133)  time: 0.3874  data: 0.0047  max mem: 11250
Epoch: [0]  [ 660/7329]  eta: 0:43:50  lr: 0.006613  real_losses_super: 1.3824 (1.7774)  real_losses_base: 1.3800 (1.7804)  losses_base: 1.3847 (1.7935)  kd_loss: 0.0050 (0.0131)  time: 0.3877  data: 0.0043  max mem: 11257
Epoch: [0]  [ 680/7329]  eta: 0:43:41  lr: 0.006813  real_losses_super: 1.3177 (1.7646)  real_losses_base: 1.3462 (1.7680)  losses_base: 1.3518 (1.7808)  kd_loss: 0.0046 (0.0129)  time: 0.3869  data: 0.0044  max mem: 11257
Epoch: [0]  [ 700/7329]  eta: 0:43:31  lr: 0.007013  real_losses_super: 1.2946 (1.7519)  real_losses_base: 1.3134 (1.7556)  losses_base: 1.3168 (1.7682)  kd_loss: 0.0049 (0.0127)  time: 0.3848  data: 0.0047  max mem: 11257
Epoch: [0]  [ 720/7329]  eta: 0:43:22  lr: 0.007213  real_losses_super: 1.2893 (1.7405)  real_losses_base: 1.3020 (1.7445)  losses_base: 1.3062 (1.7570)  kd_loss: 0.0049 (0.0125)  time: 0.3854  data: 0.0047  max mem: 11257
Epoch: [0]  [ 740/7329]  eta: 0:43:14  lr: 0.007413  real_losses_super: 1.3081 (1.7308)  real_losses_base: 1.3344 (1.7350)  losses_base: 1.3431 (1.7473)  kd_loss: 0.0055 (0.0123)  time: 0.3935  data: 0.0048  max mem: 11257
Epoch: [0]  [ 760/7329]  eta: 0:43:05  lr: 0.007612  real_losses_super: 1.3684 (1.7224)  real_losses_base: 1.3842 (1.7269)  losses_base: 1.3916 (1.7391)  kd_loss: 0.0064 (0.0121)  time: 0.3913  data: 0.0049  max mem: 11257
Epoch: [0]  [ 780/7329]  eta: 0:42:57  lr: 0.007812  real_losses_super: 1.3602 (1.7128)  real_losses_base: 1.3856 (1.7177)  losses_base: 1.3911 (1.7296)  kd_loss: 0.0049 (0.0119)  time: 0.3916  data: 0.0050  max mem: 11257
Epoch: [0]  [ 800/7329]  eta: 0:42:48  lr: 0.008012  real_losses_super: 1.3812 (1.7061)  real_losses_base: 1.4016 (1.7113)  losses_base: 1.4078 (1.7231)  kd_loss: 0.0053 (0.0118)  time: 0.3885  data: 0.0045  max mem: 11257
Epoch: [0]  [ 820/7329]  eta: 0:42:39  lr: 0.008212  real_losses_super: 1.3688 (1.6980)  real_losses_base: 1.3782 (1.7034)  losses_base: 1.3876 (1.7151)  kd_loss: 0.0059 (0.0117)  time: 0.3858  data: 0.0046  max mem: 11257
Epoch: [0]  [ 840/7329]  eta: 0:42:31  lr: 0.008412  real_losses_super: 1.3156 (1.6885)  real_losses_base: 1.3308 (1.6941)  losses_base: 1.3353 (1.7056)  kd_loss: 0.0056 (0.0115)  time: 0.3900  data: 0.0046  max mem: 11257
Epoch: [0]  [ 860/7329]  eta: 0:42:22  lr: 0.008611  real_losses_super: 1.2314 (1.6784)  real_losses_base: 1.2486 (1.6844)  losses_base: 1.2528 (1.6958)  kd_loss: 0.0061 (0.0114)  time: 0.3872  data: 0.0047  max mem: 11257
Epoch: [0]  [ 880/7329]  eta: 0:42:14  lr: 0.008811  real_losses_super: 1.3270 (1.6705)  real_losses_base: 1.3452 (1.6769)  losses_base: 1.3514 (1.6882)  kd_loss: 0.0064 (0.0113)  time: 0.3877  data: 0.0049  max mem: 11257
Epoch: [0]  [ 900/7329]  eta: 0:42:05  lr: 0.009011  real_losses_super: 1.3643 (1.6637)  real_losses_base: 1.3662 (1.6701)  losses_base: 1.3831 (1.6813)  kd_loss: 0.0098 (0.0113)  time: 0.3890  data: 0.0046  max mem: 11257
Epoch: [0]  [ 920/7329]  eta: 0:41:57  lr: 0.009211  real_losses_super: 1.4526 (1.6592)  real_losses_base: 1.4679 (1.6660)  losses_base: 1.4782 (1.6772)  kd_loss: 0.0089 (0.0113)  time: 0.3880  data: 0.0044  max mem: 11257
Epoch: [0]  [ 940/7329]  eta: 0:41:48  lr: 0.009411  real_losses_super: 1.3652 (1.6537)  real_losses_base: 1.3889 (1.6607)  losses_base: 1.3972 (1.6719)  kd_loss: 0.0073 (0.0112)  time: 0.3896  data: 0.0048  max mem: 11257
Epoch: [0]  [ 960/7329]  eta: 0:41:40  lr: 0.009610  real_losses_super: 1.2538 (1.6459)  real_losses_base: 1.2765 (1.6531)  losses_base: 1.2818 (1.6643)  kd_loss: 0.0069 (0.0111)  time: 0.3921  data: 0.0048  max mem: 11257
Epoch: [0]  [ 980/7329]  eta: 0:41:31  lr: 0.009810  real_losses_super: 1.2473 (1.6382)  real_losses_base: 1.2601 (1.6456)  losses_base: 1.2659 (1.6567)  kd_loss: 0.0066 (0.0111)  time: 0.3819  data: 0.0046  max mem: 11257
Epoch: [0]  [1000/7329]  eta: 0:41:23  lr: 0.010000  real_losses_super: 1.4298 (1.6347)  real_losses_base: 1.4478 (1.6424)  losses_base: 1.4552 (1.6534)  kd_loss: 0.0060 (0.0110)  time: 0.3913  data: 0.0050  max mem: 11257
Epoch: [0]  [1020/7329]  eta: 0:41:15  lr: 0.010000  real_losses_super: 1.3523 (1.6305)  real_losses_base: 1.4127 (1.6384)  losses_base: 1.4203 (1.6493)  kd_loss: 0.0073 (0.0109)  time: 0.3884  data: 0.0046  max mem: 11257
Epoch: [0]  [1040/7329]  eta: 0:41:06  lr: 0.010000  real_losses_super: 1.5497 (1.6290)  real_losses_base: 1.5507 (1.6368)  losses_base: 1.5809 (1.6485)  kd_loss: 0.0373 (0.0117)  time: 0.3884  data: 0.0048  max mem: 11257
Epoch: [0]  [1060/7329]  eta: 0:40:58  lr: 0.010000  real_losses_super: 1.6239 (1.6285)  real_losses_base: 1.6465 (1.6364)  losses_base: 1.6878 (1.6489)  kd_loss: 0.0306 (0.0125)  time: 0.3877  data: 0.0047  max mem: 11257
Epoch: [0]  [1080/7329]  eta: 0:40:50  lr: 0.010000  real_losses_super: 1.5340 (1.6298)  real_losses_base: 1.5227 (1.6363)  losses_base: 1.5714 (1.6507)  kd_loss: 0.0340 (0.0144)  time: 0.3932  data: 0.0045  max mem: 11257
Epoch: [0]  [1100/7329]  eta: 0:40:42  lr: 0.010000  real_losses_super: 1.8390 (1.6354)  real_losses_base: 1.8373 (1.6414)  losses_base: 1.9423 (1.6688)  kd_loss: 0.1085 (0.0274)  time: 0.3860  data: 0.0047  max mem: 11257
real_loss_super is nan, stopping training
[2024-04-28 21:30:48,477] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 3979965) of binary: /home/hslee/anaconda3/bin/python
Traceback (most recent call last):
  File "/home/hslee/anaconda3/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.2.1', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_custom.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-04-28_21:30:48
  host      : ampere
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3979966)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-04-28_21:30:48
  host      : ampere
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 3979967)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-04-28_21:30:48
  host      : ampere
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 3979968)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-04-28_21:30:48
  host      : ampere
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3979965)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
